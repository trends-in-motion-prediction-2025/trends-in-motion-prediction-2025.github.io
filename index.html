<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Trends in Motion Prediction Toward Deployable and Generalizable Autonomy:
    A Revisit and Perspectives">
  <meta name="keywords" content="Motion Prediction, Autonomous Driving">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Trends in Motion Prediction Toward Deployable and Generalizable Autonomy:
    A Revisit and Perspectives</title>
  <style>
    .title {
        white-space: nowrap;
    }
    .highlight,
    .highlight * {          /* 把内部元素也一并设色 */
      color: #4873c2 !important;
    }

    .section {
        margin-top: -160px; /* Adjust the value as needed */
    }
    .reduce-distance {
            margin-bottom: 10px; /* Specific margin for reducing distance */
        }
    .hero-body {
        display: flex;
        flex-direction: column;
        align-items: center;
        text-align: center; /* Ensures the text is centered */
        margin-bottom: 60px; /* Adjust this value as needed */
    }
    .hero {
            margin-bottom: -80px; /* Adjust this value as needed */
        }
    /* 让所有 Bulma 容器在大屏全宽 */
    /* @media screen and (min-width: 1024px) {
      .container {
        max-width: 80% !important;
      }
    } */
  </style>

<script>
  document.addEventListener("DOMContentLoaded", () => {
    // 选中所有可折叠标题
    document.querySelectorAll(".is-collapsible-title").forEach(title => {
      title.addEventListener("click", () => {
        title.classList.toggle("is-open");                   // 箭头旋转
        const body = title.nextElementSibling;               // 紧跟着的内容块
        if (body && body.classList.contains("is-collapsible-content")) {
          body.style.display = (body.style.display === "none" || !body.style.display) ? "block" : "none";
        }
      });
    });
  });
  </script>
  

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <!-- <img src="./static/images/logo.png" alt="DG Motion Prediction Logo" width="120"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    /* 折叠用的公共样式 */
    .is-collapsible-title {
      cursor: pointer;
      user-select: none;
      text-align: left !important;  /* 打败 Bulma 的 .has-text-centered */
      width: 100%;                  /* 整行可点，视觉也靠左 */
      margin-top: 1.5rem;
    }
    .is-collapsible-title::after {           /* ⬇ / ⬆ 指示箭头 */
      content: " ▸";
      transition: transform 0.2s;
    }
    .is-collapsible-title.is-open::after {
      transform: rotate(90deg);
    }
    .is-collapsible-content {
      display: none;                         /* 默认收起 */
      overflow: hidden;
    }
  </style>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> 
            <span class="highlight">Trends in Motion Prediction Toward <br>Deployable and Generalizable Autonomy:</br></span> 
            A Revisit and Perspectives
          </h1>
          <!-- <div class="is-size-4 publication-authors">
            <span class="univerity-block">
              <strong style="color: #85B737;">NeurIPS 2024</strong>
            </span>
          </div> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="https://letian-wang.github.io/">Letian Wang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.ca/citations?user=Pw0nWeYAAAAJ&hl=en">Marc-Antoine Lavoie</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=uSc-h44AAAAJ&hl=en">Sandro Papais</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7AcdtMcAAAAJ&hl=en">Barza Nisar</a><sup>1</sup>,</span><br>
              <span class="author-block">
                <a href="https://research.nvidia.com/person/yuxiao-chen">Yuxiao Chen</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://wenhao.pub/">Wenhao Ding</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.borisivanovic.com/">Boris Ivanovic</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://hao-shao.com/">Hao Shao</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://walleclipse.github.io/">Abulikemu Abuduweili</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="https://www.trailab.utias.utoronto.ca/evan-cook">Evan Cook</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=cA9lRBcAAAAJ&hl=en">Yang Zhou</a><sup>1</sup>,</span><br>
              <span class="author-block">
                <a href="https://karkus.tilda.ws/">Peter Karkus</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://jiachenli94.github.io/">Jiachen Li</a><sup>5</sup>,</span>
              <span class="author-block">
                <a href="http://icontrol.ri.cmu.edu/people/changliu.html">Changliu Liu</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="https://research.nvidia.com/person/marco-pavone">Marco Pavone</a><sup>2,6</sup>,</span>
              <span class="author-block">
                <a href="https://www.trailab.utias.utoronto.ca/steven-waslander">Steven L. Waslander</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Toronto,</span>
            <span class="author-block"><sup>2</sup>NVIDIA Research,</span>
            <span class="author-block"><sup>3</sup>Chinese University of Hong Kong,</span><br>
            <span class="author-block"><sup>4</sup>Carnegie Mellon University,</span>
            <span class="author-block"><sup>5</sup>University of California, Riverside,</span>
            <span class="author-block"><sup>6</sup>Stanford University</span>

          </div>

          <!-- <p class="is-size-5"><strong>
            *Corresponding&nbsp;Author</strong>
          </p> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.09074"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Video Link. -->
              <span class="link-block">
                <a href="" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video (Coming Soon)</span>
                </a>
              </span>

              <!-- Gitbook Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Gitbook (Coming Soon)</span>
                  </a>
              </span>

              <!-- Poster Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/distillnerf/distillnerf.github.io/tree/main/static/images/0_poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span> -->

              <!-- <span class="link-block">
                <a href="https://github.com/distillnerf/distillnerf.github.io/tree/main/static/images/0_poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                  </a>
              </span> -->

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin-bottom: 6rem;">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          
          <p><strong>
            TL;DR: Motion prediction has seen rapid progress under benchmark settings, but are we solving the right problems for real-world autonomy?
            This paper focuses on two key, often underexplored, areas in motion prediction research: deployability
            and generalizability. <span class="highlight"><strong>The goal is to develop models that can be deployed under realistic standards and
            generalize from limited seen scenarios to open-world settings</strong></span>. This is particularly relevant for
            applications like autonomous driving, robotics, and human motion analysis. We aim to emphasize shifting research focus from solely benchmark-driven progress to addressing the
            complexities and demands of real-world deployment and open-world generalization.
          </strong></p>        

        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>

<section class="section" style="margin-bottom: 1rem;">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <img src="./static/images/fig_1-vision_h.png">

          <p>
            <strong>
              Our roadmap toward deployable and generalizable models for autonomous systems is illustrated
              as above. We posit that the research community needs to revisit
              
              <!-- <span class="highlight"><strong>four key challenges in the deployable of motion prediction methods</strong></span>,
              including&nbsp;1) the design of inter-module representations,&nbsp;2) awareness of uncertainty and errors
              throughout the autonomy stack,&nbsp;3) joint learning across modules,&nbsp;and&nbsp;4) aligning the evaluation
              with the advancement of overall system performance. <span class="highlight"><strong>We introduce a generalization lifecycle that consists of four
              key stages</strong></span>, also illustrated in the roadmap: 1) lifelong data engine, 2) training, 3) safety-aware deployment and 4) test-time adaptation. -->
            </strong>
          </p>  
          
          <p><strong>
            (a) <span class="highlight"><strong>Deployable Considerations</strong></span>. Models intended for deployment in real-world autonomy should be 
            developed and evaluated under realistic settings of the closed-loop autonomoy stack, which require: (1) <span class="highlight"><strong>Representation:</strong></span> adopting inter-module
            representations that are informative, efficient, and scalable with data; (2) <span class="highlight"><strong>Uncertainty:</strong></span> quantifying, progagating, calibrating, and ultimately reduce the
             uncertainty throughout the autonomy stack; (3) <span class="highlight"><strong>Joint Learning:</strong></span> enabling joint learning across modules to promote information sharing and resolve
              incompatibilities; and (4) <span class="highlight"><strong>System-Aligned Evaluation:</strong></span> aligning evaluation with the performance of the full closed-loop system.
          </strong></p>
          
          <p><strong>
            (b) <span class="highlight"><strong>Generalization Lifecycle</strong></span>. The resulting models and evaluation protocols should then be
            integrated into a generalization cycle that includes: (1) <span class="highlight"><strong>Data Engine:</strong></span> absorbs diverse data to broaden the support of the training distribution; 
            (2) <span class="highlight"><strong>Training:</strong></span> learns representations capable of generalizing across a wide range of operational domains; 
            (3) <span class="highlight"><strong>Test-Time Robustness:</strong></span> handles sporadic distribution shifts during deployment to ensure safety; 
            and (4) <span class="highlight"><strong>Test-Time Adaptation:</strong></span> adapts online to anticipate and respond to episodic distribution changes.
            After each cycle, the lifelong learning system updates the database with newly encountered OOD data to expand the data
            coverage, thereby initiating the next generalization cycle to further expand the operational envelope of the system.
          </strong></p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>

<section class="hero teaser" style="margin-bottom: 4rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-2" style="color: #4873c2;">Deployability in Motion Prediction</h2>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">

          <!-- <h2 class="title is-4 is-collapsible-title" style="color: #4873c2;">Deploying Motion Prediction within Real-World Autonomous Systems</h2> -->
          <!-- <div class="is-collapsible-content"> -->
            <div class="content has-text-justified">
              <img src="./static/images/end_to_end.png">
            </div>

            <!-- <div class="is-collapsible-content"> -->
              <div class="content has-text-justified">
              <p><strong>
              In autonomous systems that operate in dynamic environments, such as autonomous vehicles and robotics, motion prediction
               does not act in isolation but functions as one module of closed-loop autonomy stacks by receiving upstream localization 
               and perception and informing downstream planning and control. Since the 2004 DARPA Grand Challenges, decomposing 
               autonomous systems into separate modules has become a well-established paradigm. Under this practice, benchmarks have
                been created to evaluate these modules independently, allowing researchers to develop novel and effective methods for 
                each component. Specifically, for motion prediction, existing benchmarks often provide curated and noise-free upstream 
                perception input, utilize trajectories as the interface between upstream and downstream modules, and focus solely 
                on open-loop prediction accuracy without considering the impact on downstream planning and control. This paradigm has 
                been highly successful, fostering the development of innovative methodologies and representations. 
                <span class="highlight"><strong>However, when deployed in real-world settings that can deviate from such idealized 
                training conditions, state-of-the-art motion prediction models can come with reduced effectiveness and reliability in deployment</strong></span>. 
                In this section, we discuss the approaches, challenges, and perspectives of developing motion prediction models under realistic 
                deployment standards in autonomous systems, where several key problems emerge from: 1) perception-prediction integration; 
                2) prediction-planning integration; 3) system-level closed-loop evaluation. 
                
                <!-- Click the following section titles for a quick takeaway. -->
            </strong></p>
            <div class="content has-text-justified">
              <img src="./static/images/deployable.png">
            </div>
            <!-- </div> -->
            </div>

            <p><strong>
              (CLICK the following section titles to see quick takeaways)
              <!-- (CLICK the following section titles to see <span class="highlight"><strong>our insights of deployability</strong></span>) -->
            </strong></p>

            <h2 class="title is-4 is-collapsible-title" style="color: #4873c2;">Deployability: Perception-Prediction Intergration</h2>
            <div class="is-collapsible-content">
              <div class="content has-text-justified">
                <p><strong>
                  In typical autonomy stacks, perception provides essential inputs for motion prediction to infer the
                  future evolution of objects or a scene. <span class="highlight"><strong>The seamless integration between perception and prediction is
                  crucial for efficient information propagation and overall system safety and performance</strong></span>. This integration
                  requires 1) <span class="highlight"><strong>Representation:</strong></span> proper design of representations and the interface between the two modules that enable smooth
                  information propagation and efficient computation, such as trajectories, occupancy, latent feature, raw sensor data such as videos and point clouds;
                  2) <span class="highlight"><strong>Uncertainty:</strong></span> propagating, calibrating, and eventually reducing
                  the uncertainties and errors from upstream perception (e.g. detection, tracking, mapping) to enable robust prediction; 
                  3) <span class="highlight"><strong>Joint Learning:</strong></span> joint learning and
                  temporal information fusion of the perception and prediction to facilitating information sharing and align the optimization
                  to the performance of the overall system. These concepts are illustrated in the folowing figure and are explored
                  in greater details in the paper.
                </strong></p>
                <div class="content has-text-justified">
                  <img src="./static/images/fig_17-perception_integration.png">
                </div>
              </div>
            </div>
            
            <h2 class="title is-4 is-collapsible-title" style="color: #4873c2;">Deployability: Prediction-Planning Intergration</h2>
            <div class="is-collapsible-content">
              <div class="content has-text-justified">
                <p><strong>
                  In typical autonomy architectures, 
                  motion prediction provides essential inputs for motion planning to generate safe and efficient ego trajectories.
                  As we approach the end of the autonomy stack, it introduces additional design desiderata beyond accuracy, 
                  <span class="highlight"><strong>placing greater emphasis on compatibility with planning algorithms and real-world closed-loop system performance and safety.</strong></span>
                  Achieving seamless integration with planning involves 1) <span class="highlight"><strong>Representation:</strong></span> choosing suitable representations at the prediction–planning
                  interface that enable effective information propagation, computational efficiency, and compatibility with downstream
                  modules, such as trajectories, occupancy, latent features, motion maneuver/skills, and sensor-level representations; 
                  2) <span class="highlight"><strong>Uncertainty:</strong></span> it also requires properly propagating, calibrating, and ultimately reducing the uncertainties and errors 
                  from prediction to support robust and risk-aware planning; 
                  3) <span class="highlight"><strong>Joint Learning:</strong></span> furthermore, joint learning of prediction and planning
                  encourages the design of prediction outputs that are better structured for direct consumption by planning algorithms, which is especially crutical in interaction-intense scenarios.
                  These concepts are illustrated in the followin figure and are explored in greater detail in the paper.
                </strong></p>
                <div class="content has-text-justified">
                  <img src="./static/images/fig_19-planning_integration.png">
                </div>
              </div>
            </div>

            <h2 class="title is-4 is-collapsible-title" style="color: #4873c2;">Deployability: Evaluation</h2>
            <div class="is-collapsible-content">
              <div class="content has-text-justified">
                <p><strong>
                  As we just discussed above, in realistic deployment, motion prediction does not operate in isolation, but functions as a critical
                  module within the closed-loop autonomy stack, receiving inputs from upstream localization and perception,
                  and informing downstream planning and control. However, existing benchmarks typically adopt a simplified
                  evaluation setting, where prediction is assessed independently of other modules and in an open-loop manner.
                  Specifically, they often assume trajectories as the only intereface, idealized and curated perception inputs without accounting for associated
                  noise and errors, and disregard how prediction results are consumed by downstream planning. Moreover,
                  evaluations are typically conducted in open-loop settings, overlooking important aspects such as temporal
                  consistency, ultimate task performance, and computation efficiency. <span class="highlight"><strong>To more accurately reflect real-world
                  performance, a realistic evaluation framework must incorporate perception-aware metrics, planning-oriented
                  metrics, and closed-loop dynamics</strong></span>, which we illustrate in the following figure and discuss in the paper, respectively.
                </strong></p>
                <div class="content has-text-justified">
                  <!-- <img src="./static/images/fig_20-evaluation.png"> -->
                  <img src="./static/images/fig_20-evaluation.png" width="700">
                </div>
              </div>
            </div>

            <h2 class="title is-4 is-collapsible-title" style="color: #4873c2;">Deployability: Future Outlooks</h2>
            <div class="is-collapsible-content">
              <div class="content has-text-justified">
                <p><strong>
                  The interplay between perception, prediction, and planning demonstrates that motion
                  prediction cannot be treated in isolation. Instead, it requires holistic approaches that account for intermodule information 
                  sharing and compatibility, the cascading effects of errors, and alignment with systemlevel performance. Despite significant 
                  advancements in applying motion prediction to real-world robotics
                  applications, several challenges persist that present <span class="highlight"><strong>opportunities for future research</strong></span>. We list some of them here and discuss them in greater details in the paper.
                </strong></p>
                
                <p><strong>
                  <span class="highlight"><strong>Holistic Integration</strong></span>: We are revolutionizing motion prediction by integrating it seamlessly into the full autonomy stack, 
                  moving beyond isolated components. This means tackling cascading errors from upstream perception and ensuring alignment with 
                  ultimate system performance, requiring holistic approaches for robust, real-world deployment.
                </strong></p>
            
                <p><strong>
                  <span class="highlight"><strong>Real-World Evaluation</strong></span>: Bridging the gap between idealized lab tests and challenging real-world conditions is paramount. 
                  Evaluation must account for uncertainties, reflect downstream utility for planners, and prioritize temporal consistency, 
                  computation latency, and task-oriented metrics. New benchmarks are essential to capture this complexity.
                </strong></p>
            
                <p><strong>
                  <span class="highlight"><strong>Task-Oriented Performance</strong></span>: Success isn't just about prediction accuracy, but about enabling ultimate task completion – 
                  like a vehicle reaching its destination safely and efficiently. This demands confident, well-calibrated predictions that 
                  support effective planning, revealing critical issues missed by traditional metrics.
                </strong></p>
            
                <p><strong>
                  <span class="highlight"><strong>End-to-End Autonomy</strong></span>: Exploring unified, data-driven frameworks that integrate perception, prediction, and planning offers immense 
                  potential. This trend aims for streamlined pipelines, enhanced scalability, and improved adaptability, promising breakthroughs in 
                  performance, reliability, and robustness.
                </strong></p>
            
                <p><strong>
                  <span class="highlight"><strong>Foundation Model Power</strong></span>: Harnessing the capabilities of large foundation models is key to advancing both modular and end-to-end systems. 
                  Overcoming the challenge of collecting scarce motion data requires innovative strategies, alongside developing methods for efficient 
                  deployment under real-time constraints.
                </strong></p>
              </div>
            </div>
            
    </div>
    </div>
  </div>
</section>


<section class="hero teaser" style="margin-bottom: 4rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-2" style="color: #4873c2;">Generalization in Motion Prediction</h2>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        
        <div class="content has-text-justified">
          <p><strong>
            When autonomous systems, such as robots and self-driving cars, are deployed in the real world, they
            will <span class="highlight"><strong>encounter diverse scenarios varying in many factors such as environmental geometries, new
            types of agents, unexpected events (weather, accidents) and adversarial attacks</strong></span>. In such contexts, the
            models can often be exposed to data that is outside the model's training distribution, where the model can
            have a significant drop in the performance or even fail to generate reasonable results. A straight-forward
            solution is to extend the training dataset to cover a wide range of operating scenarios through extensive
            data collection and labeling. While data collection has proven highly effective in fields like computer vision
            and natural language processing, where large-scale datasets can be scraped relatively easily, collecting and
            annotating data for robots and self-driving cars remains costly, inefficient, and often unable to capture
            the full diversity of possible events. <span class="highlight"><strong>This challenge highlights the need for complementary approaches
            that enhance generalization and fast adaptation</strong></span>—just as humans generalize from past experiences and
            quickly adapt to new environments with minimal examples, without forgetting previously acquired skills.
            <span class="highlight"><strong>Ultimately, robust generalization hinges on the seamless interplay between data, learning signals, and
            model architectures</strong></span>. To achieve goals in the generalization lifestyle, various approaches have been proposed to advance 
            the generalization capabilities of predictive models, including 1) Self-Supervised Learning, 2) Domain Generalization and Adaptation, 3) Continual Learning, 
            4) Out-of-Distribution Detection and Generalization, 5) Data augmentation and synthesis, and 6) Foundation Models. 
          </strong></p>
      </div>

      <div class="content has-text-justified">
        <img src="./static/images/generalizable.png">
      </div>

      <p><strong>
        (CLICK the following section titles to see <span class="highlight"><strong>our insights of generalization</strong></span>)
      </strong></p>

      <h2 class="title is-4 is-collapsible-title" style="color: #4873c2;">Generalization: Synphony of Data, Learning Signal, and Architecture</h2>
      <div class="is-collapsible-content">
        <div class="content has-text-justified">
          <p><strong> As in the figure below, we organize various generalization approaches according to how they differ in terms of 
            1) data access assumptions, such as the scale of the data, and access to target-domain data; 
            2) their development stage, such as being applied during pretraining, training, or test time; 
            and 3) the primary focus, such as whether they aim to learn generalizable representations through modified or additional learning
            signal, modify the model architecture for more robustness and adaptability, achieve generalization by scaling up the data,
            or they represent particular generalization tasks.
            Below we introduce the gist of each approache and discuss them in greater details in the paper. </strong></p>

            <div class="content has-text-justified">
              <img src="./static/images/fig_21-generalization_approaches_v3.png">
            </div>

          <p><strong>
            <span class="highlight"><strong>Self-Supervised Learning</strong></span>: Self-supervised (SSL) methods use proxy tasks, such as 
            trajectory reconstruction and contrastive learning, to learn informative and transferable representations in addition to the 
            original prediction supervision. 
            This can either be done sequentially, where SSL methods are used as a pretraining step, or simultaneously in multi-task learning, 
            where the model is optimized for both SSL and original supervised losses. In addition to the trajectory representation, SSL methods 
            are also applied to other representations such as point cloud forecasting, video generation, and scene reconstruction. 
          </strong></p>
      
          <p><strong>
            <span class="highlight"><strong>Domain Generalization and Adaptation</strong></span>: Domain generalization is the most 
            straightforward distribution shift setting and considers the case where a model trained one or more source datasets is tested on one 
            or more different target datasets in zero-shot. This corresponds to dropping the source pre-trained model in a new environment. 
            A common training setting involves multiple source datasets, and models, that learn generalizable features and are robust to 
            distribution shits across the source domains, can perform well on the target domain. In contrast to domain generalization, 
            domain adaptation considers the case where some target data is also available in addition to the source data. The target 
            data is often available only in a limited fashion e.g. only a handful of examples in the form of offline batch data or online 
            streaming data, or without prediction labels. This allows exploiting the target data, and the key challenge is to effectively 
            leverage the target information to maximize performance without overfitting.  
          </strong></p>
      
          <p><strong>
            <span class="highlight"><strong>Continual Learning</strong></span>: Continual learning extends domain adaptation to a sequence 
            of new datasets or to a continually changing distribution instead of a single shift. Moreover, compared to domain adaptation 
            that ignores performance in the source domain, continual learning methods also evaluate and aim to keep the performance on 
            all previously seen domains, where the challenge is to avoid forgetting past experiences as additional ones are learned.
          </strong></p>
      
          <p><strong>
            <span class="highlight"><strong>Out-of-Distribution Detection and Generalization</strong></span>: A slightly different test case 
            considers the presence of rare, difficult, or adversarial examples in the test set. These are the out-of-distribution (OOD) 
            examples. The assumption is that OOD data is so far from standard inlier data that a learned model would not generalize properly 
            as in domain generalization, and so we consider two strategies. The first is OOD detection, where the objective is to identify 
            the OOD data and reject it instead of using it as a prediction input. The second is OOD generalization, which still attempts to 
            generate outputs from the OOD input data, but often requires additional assumptions or models. In practice, this can mean training 
            with representative outliers or using additional unlearned models as a fall-back strategy to supplement the learned model when 
            faced with OOD inputs. This can be done in conjunction with OOD detection by routing detected OOD inputs to the robust model.  
          </strong></p>
      
          <p><strong>
            <span class="highlight"><strong>Data Augmentation and Synthesis</strong></span>: A promising direction to improve model generalization 
            is to enhance existing datasets through data augmentation and synthesis. Data augmentation aims to increase data diversity by applying 
            transformations or perturbations to existing examples, thereby improving the model's robustness and generalization ability.
            In motion prediction, augmentation techniques include perturbing agent trajectories, altering scene contexts, modifying initial conditions, 
            or simulating alternative agent intentions. Beyond augmentation, data synthesis further expands dataset diversity by generating new 
            examples that capture a broader range of scenarios. Synthesis can introduce challenging and rare situations—such as collisions or adversarial 
            agents—that are under-presented in the original datasets but are crucial for real-world deployment, particularly in new or unseen test settings.
            Unlike image-based tasks, motion prediction data is subject to significant structural constraints, such as agent interactions, initial configurations,
             environment layout, and temporal dynamics. Generated motions must remain physically and behaviorally plausible, respecting map constraints and 
             interaction dynamics over time. As a result, a key challenge in both data augmentation and synthesis is ensuring realism—that is, generating or modifying
              data in ways that are consistent with plausible human or agent behavior. 
          </strong></p>
        </div>
      </div>

      <h2 class="title is-4 is-collapsible-title" style="color: #4873c2;">Generalization: Future Outlooks</h2>
      <div class="is-collapsible-content">
        <div class="content has-text-justified">
          <p><strong>
            As motion prediction systems are deployed in increasingly diverse and unpredictable environments,
            generalization becomes a central challenge—requiring models not only to perform well on known domains
            but also to adapt, recover, or remain robust in the face of distributional shifts and open-world settings. 
            Several key themes that emerge from our paper persist and present <span class="highlight"><strong>opportunities for future research</strong></span>.
          </strong></p>
          
          <p><strong>
            <span class="highlight"><strong>Data Scaling and Distribution Understanding</strong></span>: Despite the demonstrated success of 
            large-scale datasets in NLP and computer vision, the field of motion prediction remains largely in a small-data regime due to the 
            costly and labor-intensive process of collecting and annotating motion data. For instance, leading autonomous driving datasets 
            such as Argoverse, Argoverse 2, and the Waymo Open Motion Dataset (WOMD) contain only 320K, 250K, and 480K data sequences, 
            respectively—orders of magnitude smaller than datasets in NLP (e.g., GPT-3's 400B-token CommonCrawl) or vision (e.g., JFT's 303M images).
            This scarcity of data constrains models’ ability to learn rich and transferable representations, ultimately limiting their robustness 
            and generalization. 
            Therefore, a critical priority is scaling up high-quality datasets, either by unifying 
            existing sources to overcome format and definition discrepancies, by developping efficient data collection pipelines, or by leveraging data synthesis methods. 
           Meanwhile, as datasets scale, simply adding more data is not sufficient. Understanding and quantifying the underlying data distribution becomes essential to ensure diversity, 
            avoid redundancy, and guide efficient data usage. Tools are required to characterize dataset coverage, diversity, and scenario 
            difficulty, moving beyond low-level statistics or handcrafted metrics. This understanding is crucial for nearly every aspect of 
            the generalization lifecycle, such as fair and informative cross-dataset benchmarking, designing efficient learning paradigms with balanced data coverage (e.g. dataset distillation, 
            active learning), evaluating the novelty of newly collected or synthetic data, and guiding the design of generalizable models themselves. 
          </strong></p>
      
          <p><strong>
            <span class="highlight"><strong>Revolutionize Modelling and Learning Strategies</strong></span>: 
            Motion prediction introduces unique structural and temporal challenges that set it apart from vision
           and language tasks. Unlike static images or discrete text, motion data consists of continuous trajectories
            shaped by physical laws, spatial constraints, and multi-agent interactions. Generalizable models in this domain
             must therefore learn representations that generalize across diverse scene layouts, agent types, behavioral patterns,
              and geographies—while respecting the causal and temporal dependencies inherent in real-world motion.
            To this end, several key questions remain open. What inductive biases, tokenization strategies, or architectural modules
             are best suited to represent agents, maps, and interactions effectively? Should representations be built on trajectory-level 
             abstractions or raw sensor streams like point clouds and videos—which bypass annotation and offer richer context, but
              introduce high computational costs and struggle to capture discrete agent behaviors? More broadly, the optimal approach 
              for learning informative, transferable features across heterogeneous motion datasets remains unclear—highlighting the 
              need for further research into motion-specific pretraining objectives, scalable architectures, and adaptation strategies.
            Ultimately, developing such models will require rethinking not only training but also deployment—encompassing how to fine-tune
            models for specific downstream tasks, and how to ensure robust generalization under distribution shifts through
              test-time OOD detection, generalization, and adaptation.
          </strong></p>
      
          <p><strong>
            <span class="highlight"><strong>Establish Standardized Benchmarks and Unified Evaluation</strong></span>: 
            Although recent works have explored generalizable motion prediction through diverse perspectives—ranging from self-supervised learning and 
            domain adaptation to continual learning, OOD detection, and foundation models—the field remains in its early stages. While progress has been made, 
            many methods are still exploratory and fragmented, hindered by the lack of standardized evaluation protocols. In particular, the absence of 
            widely accepted cross-dataset benchmarks contributes to this fragmentation: each paper often adopts its own customized setting—e.g., specific 
            data scales, shift types, or evaluation metrics—making it difficult to fairly compare methods, assess their generality, or track progress across 
            the field.
            Future efforts should focus on establishing unified and realistic evaluation protocols that span multiple tasks, 
             data regimes, and evaluation metrics. In contrast to classification, where OOD can be defined by unseen labels, benchmarking in regression tasks 
             like motion prediction requires more nuanced definitions, moving beyond coarse-grained approaches that treat entire different datasets as OOD. Evaluations 
             should also go beyond accuracy to include uncertainty estimation, which is essential for safety-critical downstream applications. Moreover, 
             fair comparisons under equal compute budgets are necessary to ensure meaningful assessment of progress.
          </strong></p>
      
          <p><strong>
            <span class="highlight"><strong>Unleash the Power of Foundation Models</strong></span>: As data understanding, generalization methods, 
            and benchmarking continue to mature, foundation models are emerging to unify and extend these capabilities—offering a path toward robust 
            generalization in the open world. This paradigm builds on transformative advances in computer vision and natural language processing, where a large-parameter model trained with 
            a large amount of data shows strong zero-shot and few-shot generalization across diverse tasks and modalities.
            This emerging trend includes two complementary directions:
            <span class="highlight"><strong>1) Developing Motion-Specific Foundation Models</strong></span>: There is a growing need to 
            design large models tailored for the structured and dynamic nature of motion data. This involves unifying diverse datasets, 
            rethinking tokenization and architectural priors, and developing adaptation strategies across agents, geographies, and scenarios.
            <span class="highlight"><strong>2) Adapting Existing Foundation Models</strong></span>: Given the relatively limited motion 
            data, adapting general-purpose foundation models (e.g., LLMs, video generation models) trained on internet-scale corpora is 
            a promising but nascent direction. These models offer emergent reasoning and common-sense capabilities that could benefit 
            motion tasks, but challenges remain in bridging modality gaps, aligning spatial-temporal reasoning with physical constraints, 
            and ensuring safe integration into real-world systems. Exploring their potential for zero- and few-shot generalization in 
            motion prediction is a key research opportunity.
      
            In essence: the unique robustness to open world distribution shifts that is natural for humans may be an emergent property of 
            large scale models, and the path to achieving human-like performance in prediction systems may be a matter of further scaling
             up large foundation models. This aligns with "Bitter Lesson", which emphasizes the simple 
             importance of scaling up resources over custom-tailored algorithms and architectures. 
          </strong></p>
        </div>
      </div>

    </div>
  </div>
</div>
</section>


<section class="hero teaser" style="margin-bottom: 2rem;">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>

    <pre><code>@misc{wang2025deployablegeneralizablemotionprediction,
  title        = {Deployable and Generalizable Motion Prediction: Taxonomy, Open Challenges and Future Directions},
  author       = {Letian Wang and
                  Marc-Antoine Lavoie and
                  Sandro Papais and
                  Barza Nisar and
                  Yuxiao Chen and
                  Wenhao Ding and
                  Boris Ivanovic and
                  Hao Shao and
                  Abulikemu Abuduweili and
                  Evan Cook and
                  Yang Zhou and
                  Peter Karkus and
                  Jiachen Li and
                  Changliu Liu and
                  Marco Pavone and
                  Steven Waslander},
  year         = {2025},
  eprint       = {2505.09074},
  archivePrefix= {arXiv},
  primaryClass = {cs.RO},
  url          = {https://arxiv.org/abs/2505.09074}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link" href="">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <!-- <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i> -->
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            We thank the authors of <a href="https://nerfies.github.io/">Nerfies</a> that kindly open sourced the
            template of this website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- <a href="https://clustrmaps.com/site/1c07g"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=hVB8BKDOSMZDbhrmbQb6Du6W4iXg4fbDmxdI-ewuKdg&cl=ffffff" /></a> -->
<div id="clustrmaps-container" style="display: none;">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=n&d=hVB8BKDOSMZDbhrmbQb6Du6W4iXg4fbDmxdI-ewuKdg'></script>
  <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=70&t=n&d=3pp5wthj_B_tqIuIxSFqmJlNrIjSTCEobtZnxNdSV7M&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script> -->
</div>
</body>
</html>
